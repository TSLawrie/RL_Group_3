{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65852dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing\n",
      "Episode  0\n",
      "Episode  1\n",
      "Episode  2\n",
      "Episode  3\n",
      "Episode  4\n",
      "Episode  5\n",
      "Episode  6\n",
      "Episode  7\n",
      "Episode  8\n",
      "Episode  9\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import random\n",
    "\n",
    "class DQNLunarLander():\n",
    "    def __init__(self, training_episodes):\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.training_episodes = training_episodes\n",
    "        self.env = gym.make('LunarLander-v2')\n",
    "        self.epsilon = 0.15\n",
    "        self.replay_memory = [] # (state, action, reward, next_state)\n",
    "        self.batch_size = 5\n",
    "        self.epochs = 1\n",
    "        self.gamma = 0.99\n",
    "        self.seed = 0\n",
    "        self.Q1 = NeuralNetwork(seed=0, action_size=4, state_size=8).to(device)\n",
    "        self.Q2 = NeuralNetwork(seed=0, action_size=4, state_size=8).to(device)\n",
    "        self.learning_rate = 1e-5\n",
    "        self.C = 100\n",
    "        \n",
    "        # Replay memory\n",
    "        # Action value network with arbitrary weights (Q1)\n",
    "        # Target action value network with same weights (Q2)\n",
    "    \n",
    "    def execute_episode(self, initial_state):\n",
    "        # Initialise state\n",
    "        \n",
    "        # For t=1 to T\n",
    "        # pick random action with p=0.15\n",
    "        \n",
    "        state = initial_state\n",
    "        state_tensor = torch.tensor(state)\n",
    "        \n",
    "        rewards = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "        \n",
    "            random_choice = np.random.uniform(0, 1)\n",
    "            if (random_choice < self.epsilon):\n",
    "                action = self.env.action_space.sample()    \n",
    "            \n",
    "            else:\n",
    "                action = self.Q1(torch.from_numpy(state)).argmax().cpu().detach().numpy()\n",
    "            \n",
    "            # pick best action from Q1\n",
    "            # Execute action At and observe reward Rt and next state St+1\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            self.replay_memory.append((state, action, reward, next_state, done))\n",
    "            rewards += reward\n",
    "\n",
    "            sample_idxs = np.random.choice(range(len(self.replay_memory)), size=self.batch_size)\n",
    "            samples = []\n",
    "            \n",
    "            for idx, s in enumerate(self.replay_memory):\n",
    "                if idx in sample_idxs:\n",
    "                    samples.append(s)\n",
    "\n",
    "            predictions = torch.tensor([])\n",
    "            current_state_values = torch.tensor([])\n",
    "            \n",
    "            # Minibatch gradient descent\n",
    "            for sample in samples:\n",
    "                (Sj, Aj, Rj, Sj_1, done_j) = sample\n",
    "\n",
    "                current_state_action_val = self.Q1(torch.from_numpy(Sj)).max().detach().numpy() # Returns the probability of predicting Aj from Sj\n",
    "                target_state_action_val = self.Q2(torch.from_numpy(Sj_1)).max().detach().numpy() # Returns best action value for Sj+1\n",
    "\n",
    "                # Set y_j = Rj + 0 if next state is terminal\n",
    "                y_j = Rj\n",
    "\n",
    "                # Rj + value otherwise\n",
    "                if (not done_j):\n",
    "                    y_j += self.gamma * target_state_action_val\n",
    "\n",
    "                # Perform gradient descent between y_j and current_state_action_val\n",
    "                \n",
    "                np.append(predictions, y_j)\n",
    "                np.append(current_state_values, current_state_action_val)\n",
    "                \n",
    "            predictions_tensor = torch.tensor(predictions.detach().numpy(), requires_grad=True)\n",
    "            current_state_vals_tensor = torch.tensor(current_state_values.detach().numpy(), requires_grad=True)\n",
    "                \n",
    "            criterion = torch.nn.MSELoss()\n",
    "            loss = criterion(predictions_tensor, current_state_vals_tensor)\n",
    "\n",
    "            optimizer = torch.optim.SGD(self.Q1.parameters(), lr=self.learning_rate)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "            # Store transition (St, At, Rt, St+1) in D\n",
    "            # Sample random minibatch transitions from D\n",
    "            # Perform gradient descent step\n",
    "            # Every C steps, update target weights\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "            # Update model weights\n",
    "            \n",
    "            \"\"\"\n",
    "            if t % self.C:\n",
    "                Q1_params = self.Q1.parameters()\n",
    "                with torch.no_grad():\n",
    "                    for idx, p in enumerate(self.Q2.parameters()):\n",
    "                        new_param = Q1_params[idx]\n",
    "                        p.copy_(new_param)\n",
    "            \"\"\"\n",
    "        return rewards\n",
    "\n",
    "    \n",
    "    def train(self, verbose = 0):\n",
    "        rewards = []\n",
    "        for ep in range(self.training_episodes):\n",
    "            if verbose:\n",
    "                print(\"Episode \", ep)\n",
    "            initial_state = self.env.reset()\n",
    "            reward = self.execute_episode(initial_state)\n",
    "            rewards.append(reward)\n",
    "        return rewards\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, seed, action_size, state_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Architecture \n",
    "        fc1_units = 128\n",
    "        fc2_units = 128\n",
    "        fc3_units = 128\n",
    "        \n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.dropout1 = nn.Dropout(p=0.6)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.dropout2 = nn.Dropout(p=0.6)\n",
    "        self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
    "        self.dropout3 = nn.Dropout(p=0.6)\n",
    "        self.fc4 = nn.Linear(fc3_units, action_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(x)\n",
    "        return self.fc4(x)\n",
    "\n",
    "print(\"Executing\")\n",
    "episodes = 10\n",
    "agent = DQNLunarLander(episodes)\n",
    "rewards = agent.train(verbose=1)\n",
    "\n",
    "plt.plot(episodes, rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071de57e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rewards' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12684/2473571256.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rewards' is not defined"
     ]
    }
   ],
   "source": [
    "print(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
